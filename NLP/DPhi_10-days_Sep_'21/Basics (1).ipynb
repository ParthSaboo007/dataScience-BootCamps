{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "name": "Basics.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xx128MzI9gAc"
      },
      "source": [
        "Including necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vp-GWNob9gAf",
        "outputId": "9e06c2e1-4527-4118-f6b5-a93fc789e000"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PJQL7SpO9gAi",
        "outputId": "dc996cd9-fb7c-4ee2-9560-9ea287753417"
      },
      "source": [
        "!pip install contractions\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting contractions\n",
            "  Downloading contractions-0.0.52-py2.py3-none-any.whl (7.2 kB)\n",
            "Collecting textsearch>=0.0.21\n",
            "  Downloading textsearch-0.0.21-py2.py3-none-any.whl (7.5 kB)\n",
            "Collecting anyascii\n",
            "  Downloading anyascii-0.3.0-py3-none-any.whl (284 kB)\n",
            "\u001b[K     |████████████████████████████████| 284 kB 5.1 MB/s \n",
            "\u001b[?25hCollecting pyahocorasick\n",
            "  Downloading pyahocorasick-1.4.2.tar.gz (321 kB)\n",
            "\u001b[K     |████████████████████████████████| 321 kB 53.5 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyahocorasick\n",
            "  Building wheel for pyahocorasick (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyahocorasick: filename=pyahocorasick-1.4.2-cp37-cp37m-linux_x86_64.whl size=85454 sha256=46fdee2b780eb1646b6fa2820050c97141c904bd2d194556304e050cabc37efb\n",
            "  Stored in directory: /root/.cache/pip/wheels/25/19/a6/8f363d9939162782bb8439d886469756271abc01f76fbd790f\n",
            "Successfully built pyahocorasick\n",
            "Installing collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
            "Successfully installed anyascii-0.3.0 contractions-0.0.52 pyahocorasick-1.4.2 textsearch-0.0.21\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g2vHR4RQ9pE_",
        "outputId": "dae9c3a8-7d9e-4888-c558-887288880b8f"
      },
      "source": [
        "!pip install textsearch"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: textsearch in /usr/local/lib/python3.7/dist-packages (0.0.21)\n",
            "Requirement already satisfied: anyascii in /usr/local/lib/python3.7/dist-packages (from textsearch) (0.3.0)\n",
            "Requirement already satisfied: pyahocorasick in /usr/local/lib/python3.7/dist-packages (from textsearch) (1.4.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WzSzUylo9gAj"
      },
      "source": [
        "# Case Conversion"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1dhEfNq99gAj",
        "outputId": "ebbd6c53-9629-4dfd-e24c-b0bc0d5b29b3"
      },
      "source": [
        "t1 = 'the quick Brown fox jumped over the FENCE'\n",
        "print(t1.lower())\n",
        "print(t1.upper())\n",
        "print(t1.title())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the quick brown fox jumped over the fence\n",
            "THE QUICK BROWN FOX JUMPED OVER THE FENCE\n",
            "The Quick Brown Fox Jumped Over The Fence\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HB5s5bux9gAk"
      },
      "source": [
        "# TOKENiZATiON"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "7ZoHHhjL9gAl",
        "outputId": "0f37df72-069b-4063-9838-37de5ed2959f"
      },
      "source": [
        "t2 =        (\"US unveils world's most powerful supercomputer, beats China. \" \n",
        "               \"The US has unveiled the world's most powerful supercomputer called 'Summit', \" \n",
        "               \"beating the previous record-holder China's Sunway TaihuLight. With a peak performance \"\n",
        "               \"of 200,000 trillion calculations per second, it is over twice as fast as Sunway TaihuLight, \"\n",
        "               \"which is capable of 93,000 trillion calculations per second. Summit has 4,608 servers, \"\n",
        "               \"which reportedly take up the size of two tennis courts.\")\n",
        "t2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"US unveils world's most powerful supercomputer, beats China. The US has unveiled the world's most powerful supercomputer called 'Summit', beating the previous record-holder China's Sunway TaihuLight. With a peak performance of 200,000 trillion calculations per second, it is over twice as fast as Sunway TaihuLight, which is capable of 93,000 trillion calculations per second. Summit has 4,608 servers, which reportedly take up the size of two tennis courts.\""
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xdj4yo7_9gAl",
        "outputId": "92f7f975-8c3a-4890-b982-fb43e90ccc9f"
      },
      "source": [
        "import nltk\n",
        "nltk.sent_tokenize(t2) # default tokenizer to split strings into \"SENTENCES\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"US unveils world's most powerful supercomputer, beats China.\",\n",
              " \"The US has unveiled the world's most powerful supercomputer called 'Summit', beating the previous record-holder China's Sunway TaihuLight.\",\n",
              " 'With a peak performance of 200,000 trillion calculations per second, it is over twice as fast as Sunway TaihuLight, which is capable of 93,000 trillion calculations per second.',\n",
              " 'Summit has 4,608 servers, which reportedly take up the size of two tennis courts.']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KBPEuqT99gAm",
        "outputId": "911c0745-bf79-48b7-a51f-67721a9af3a8"
      },
      "source": [
        "print(nltk.word_tokenize(t2)) # tokenizer for extracting each symbol and word"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['US', 'unveils', 'world', \"'s\", 'most', 'powerful', 'supercomputer', ',', 'beats', 'China', '.', 'The', 'US', 'has', 'unveiled', 'the', 'world', \"'s\", 'most', 'powerful', 'supercomputer', 'called', \"'Summit\", \"'\", ',', 'beating', 'the', 'previous', 'record-holder', 'China', \"'s\", 'Sunway', 'TaihuLight', '.', 'With', 'a', 'peak', 'performance', 'of', '200,000', 'trillion', 'calculations', 'per', 'second', ',', 'it', 'is', 'over', 'twice', 'as', 'fast', 'as', 'Sunway', 'TaihuLight', ',', 'which', 'is', 'capable', 'of', '93,000', 'trillion', 'calculations', 'per', 'second', '.', 'Summit', 'has', '4,608', 'servers', ',', 'which', 'reportedly', 'take', 'up', 'the', 'size', 'of', 'two', 'tennis', 'courts', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5uvGxhDA9gAn"
      },
      "source": [
        "import spacy\n",
        "from spacy.lang.en.examples import sentences \n",
        "nlp = spacy.load(\"en\")  # loads small english model\n",
        "\n",
        "text_spacy = nlp(t2)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GvNu0ugn_PXV"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nZVsQE2F_QZx"
      },
      "source": [
        "### For more Spacy content , refer this notebook: https://www.kaggle.com/hassanamin/learning-spacy-basics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "awJdLCBm-DgF",
        "outputId": "06914e74-cf7d-432f-da04-7cefb3fe6d3a"
      },
      "source": [
        "[obj.text for obj in text_spacy.sents]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"US unveils world's most powerful supercomputer, beats China.\",\n",
              " \"The US has unveiled the world's most powerful supercomputer called 'Summit', beating the previous record-holder China's Sunway TaihuLight.\",\n",
              " 'With a peak performance of 200,000 trillion calculations per second, it is over twice as fast as Sunway TaihuLight, which is capable of 93,000 trillion calculations per second.',\n",
              " 'Summit has 4,608 servers, which reportedly take up the size of two tennis courts.']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8vm1QjIZ_lZd",
        "outputId": "566682c9-f02a-435b-a8d6-971dfdd6a29f"
      },
      "source": [
        "print([obj.text for obj in text_spacy])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['US', 'unveils', 'world', \"'s\", 'most', 'powerful', 'supercomputer', ',', 'beats', 'China', '.', 'The', 'US', 'has', 'unveiled', 'the', 'world', \"'s\", 'most', 'powerful', 'supercomputer', 'called', \"'\", 'Summit', \"'\", ',', 'beating', 'the', 'previous', 'record', '-', 'holder', 'China', \"'s\", 'Sunway', 'TaihuLight', '.', 'With', 'a', 'peak', 'performance', 'of', '200,000', 'trillion', 'calculations', 'per', 'second', ',', 'it', 'is', 'over', 'twice', 'as', 'fast', 'as', 'Sunway', 'TaihuLight', ',', 'which', 'is', 'capable', 'of', '93,000', 'trillion', 'calculations', 'per', 'second', '.', 'Summit', 'has', '4,608', 'servers', ',', 'which', 'reportedly', 'take', 'up', 'the', 'size', 'of', 'two', 'tennis', 'courts', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UQhGz5Pd-0tS",
        "outputId": "b91f103a-dd7d-4188-830c-fc6290d922e8"
      },
      "source": [
        "for token in text_spacy:\n",
        "  # printing the text and predited part-of-speech tag\n",
        "  print(token.text, token.pos_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "US PROPN\n",
            "unveils VERB\n",
            "world NOUN\n",
            "'s PART\n",
            "most ADV\n",
            "powerful ADJ\n",
            "supercomputer NOUN\n",
            ", PUNCT\n",
            "beats VERB\n",
            "China PROPN\n",
            ". PUNCT\n",
            "The DET\n",
            "US PROPN\n",
            "has AUX\n",
            "unveiled VERB\n",
            "the DET\n",
            "world NOUN\n",
            "'s PART\n",
            "most ADV\n",
            "powerful ADJ\n",
            "supercomputer NOUN\n",
            "called VERB\n",
            "' PUNCT\n",
            "Summit PROPN\n",
            "' PUNCT\n",
            ", PUNCT\n",
            "beating VERB\n",
            "the DET\n",
            "previous ADJ\n",
            "record NOUN\n",
            "- PUNCT\n",
            "holder NOUN\n",
            "China PROPN\n",
            "'s PART\n",
            "Sunway PROPN\n",
            "TaihuLight PROPN\n",
            ". PUNCT\n",
            "With ADP\n",
            "a DET\n",
            "peak NOUN\n",
            "performance NOUN\n",
            "of ADP\n",
            "200,000 NUM\n",
            "trillion NUM\n",
            "calculations NOUN\n",
            "per ADP\n",
            "second NOUN\n",
            ", PUNCT\n",
            "it PRON\n",
            "is AUX\n",
            "over ADV\n",
            "twice ADV\n",
            "as ADV\n",
            "fast ADV\n",
            "as SCONJ\n",
            "Sunway PROPN\n",
            "TaihuLight PROPN\n",
            ", PUNCT\n",
            "which DET\n",
            "is AUX\n",
            "capable ADJ\n",
            "of ADP\n",
            "93,000 NUM\n",
            "trillion NUM\n",
            "calculations NOUN\n",
            "per ADP\n",
            "second NOUN\n",
            ". PUNCT\n",
            "Summit NOUN\n",
            "has AUX\n",
            "4,608 NUM\n",
            "servers NOUN\n",
            ", PUNCT\n",
            "which DET\n",
            "reportedly ADV\n",
            "take VERB\n",
            "up ADP\n",
            "the DET\n",
            "size NOUN\n",
            "of ADP\n",
            "two NUM\n",
            "tennis NOUN\n",
            "courts NOUN\n",
            ". PUNCT\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nQxupdx0_FBE"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n8IrQteq9gAn"
      },
      "source": [
        "# Removing HTML tags and noises"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yVCif4E09gAo",
        "outputId": "456dadb8-1c46-4cf9-d169-fcc1364ee80f"
      },
      "source": [
        "import requests\n",
        "\n",
        "data = requests.get('http://www.gutenberg.org/cache/epub/8001/pg8001.html')\n",
        "content = data.text\n",
        "print(content[2745:3948])\n",
        "# displaying random html data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "%;\t\t\t/* adjust to ape original work */\r\n",
            "\t\tmargin-top: 1em;\t/* space above &amp; below */\r\n",
            "\t\tmargin-bottom: 1em;\r\n",
            "\t\tmargin-left: auto;  /* these two ensure a.. */\r\n",
            "\t\tmargin-right: auto; /* ..centered rule */\r\n",
            "\t\tclear: both;\t\t/* don't let sidebars &amp; floats overlap rule */\r\n",
            "\t}\r\n",
            "/* ************************************************************************\r\n",
            " * Images and captions\r\n",
            " * ********************************************************************** */\r\n",
            "\timg { /* the default inline image has */\r\n",
            "\t\tborder: 1px solid black; /* a thin black line border.. */\r\n",
            "\t\tpadding: 6px; /* ..spaced a bit out from the graphic */\r\n",
            "\t\t} </style>\r\n",
            "<link rel=\"schema.dc\" href=\"http://purl.org/dc/elements/1.1/\">\r\n",
            "<link rel=\"schema.dcterms\" href=\"http://purl.org/dc/terms/\">\r\n",
            "<meta name=\"dc.title\" content=\"The Bible, King James version, Book 1: Genesis\">\r\n",
            "<meta name=\"dc.language\" content=\"en\">\r\n",
            "<meta name=\"dcterms.source\" content=\"https://www.gutenberg.org/files/8001/8001.txt\">\r\n",
            "<meta name=\"dcterms.modified\" content=\"2021-09-04T09:42:22.231886+00:00\">\r\n",
            "<meta name=\"dc.rights\" content=\"Public domain in the USA.\">\r\n",
            "<link rel=\"dcterms.isFormatOf\" href=\"http://www.gutenberg.org/ebooks/8001\">\r\n",
            "<meta name=\"dc.creato\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B1Wm7E709gAo",
        "outputId": "55bd6e47-b1fd-4cd6-d46a-cdff9fe9e4d2"
      },
      "source": [
        "import re\n",
        "from bs4 import BeautifulSoup # necessary library\n",
        "\n",
        "def strip_html_tags(text):\n",
        "    soup = BeautifulSoup(text, \"html.parser\")\n",
        "    [s.extract() for s in soup(['iframe', 'script'])]\n",
        "    stripped_text = soup.get_text()\n",
        "    stripped_text = re.sub(r'[\\r|\\n|\\r\\n]+', '\\n', stripped_text)\n",
        "    return stripped_text\n",
        "\n",
        "clean_content = strip_html_tags(content) # passing our html content into the function\n",
        "print(clean_content[1000:4000])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "in &lt;div&gt; or &lt;table&gt;  */\n",
            "\t\ttext-align: justify; /* or left?? */\n",
            "\t\ttext-indent: 1em;\t/* first-line indent */\n",
            "\t}\n",
            "\t/* suppress indentation on paragraphs following heads  */\n",
            "\th2+p, h3+p, h4+p { text-indent: 0; }\n",
            "\t/* tighter spacing for list item paragraphs */\n",
            "\tdd, li {\n",
            "\t\tmargin-top: 0.25em; margin-bottom:0;\n",
            "\t\tline-height: 1.2em; /* a bit closer than p's */\n",
            "\t}\n",
            "/* ************************************************************************\n",
            " * Head 2 is for chapter heads. \n",
            " * ********************************************************************** */\n",
            "\th2 {\n",
            "\t\t/* text-align:center;  left-aligned by default. */\n",
            "\t\tmargin-top:3em;\t\t/* extra space above.. */\n",
            "\t\tmargin-bottom: 2em;\t/* ..and below */\n",
            "\t\tclear: both;\t\t/* don't let sidebars overlap */\n",
            "\t}\n",
            "/* ************************************************************************\n",
            " * Head 3 is for main-topic heads.\n",
            " * ********************************************************************** */\n",
            "\th3 {\n",
            "\t\t\t/* text-align:center;  left-aligned by default. */\n",
            "\t\t\tmargin-top: 2em;\t/* extra space above but not below */\n",
            "\t\t\tfont-weight: normal; /* override default of bold */\n",
            "\t\t\tclear: both; /* don't let sidebars overlap */\n",
            "\t}\n",
            "/* ************************************************************************\n",
            " * Styling the default HR and some special-purpose ones.\n",
            " * Default rule centered and clear of floats; sized for thought-breaks\n",
            " * ********************************************************************** */\n",
            "\thr {\n",
            "\t\twidth:45%;\t\t\t/* adjust to ape original work */\n",
            "\t\tmargin-top: 1em;\t/* space above &amp; below */\n",
            "\t\tmargin-bottom: 1em;\n",
            "\t\tmargin-left: auto;  /* these two ensure a.. */\n",
            "\t\tmargin-right: auto; /* ..centered rule */\n",
            "\t\tclear: both;\t\t/* don't let sidebars &amp; floats overlap rule */\n",
            "\t}\n",
            "/* ************************************************************************\n",
            " * Images and captions\n",
            " * ********************************************************************** */\n",
            "\timg { /* the default inline image has */\n",
            "\t\tborder: 1px solid black; /* a thin black line border.. */\n",
            "\t\tpadding: 6px; /* ..spaced a bit out from the graphic */\n",
            "\t\t} \n",
            "Project Gutenberg EBook The Bible, King James, Book 1: Genesis\n",
            "Copyright laws are changing all over the world. Be sure to check the\n",
            "copyright laws for your country before downloading or redistributing\n",
            "this or any other Project Gutenberg eBook.\n",
            "This header should be the first thing seen when viewing this Project\n",
            "Gutenberg file.  Please do not remove it.  Do not change or edit the\n",
            "header without written permission.\n",
            "Please read the \"legal small print,\" and other information about the\n",
            "eBook and Project Gutenberg at the bottom of this file.  Included is\n",
            "important information about your specific rights and restrictions in\n",
            "how the file may be used.  You can also find out about how to make a\n",
            "donation to Project Gutenberg, and how to get involved.\n",
            "**Welcome To The World of Free Plain Vanilla Electronic Texts**\n",
            "**EBooks Readable By Both Humans and By Computers, Since 1971**\n",
            "*****These EBooks Were Prepared By Thousands of \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TkuDenrx9gAp"
      },
      "source": [
        "# Removing Accented Characters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R3zpC0mf9gAp"
      },
      "source": [
        "import unicodedata\n",
        "\n",
        "# required method for operation\n",
        "def remove_accented_chars(text):\n",
        "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
        "    return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "zhxJGY0Z9gAq",
        "outputId": "a741386d-c72c-492e-8f31-9aff2e110647"
      },
      "source": [
        "t3 ='Sómě Áccěntěd těxt'\n",
        "t3"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Sómě Áccěntěd těxt'"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "RNOnXHbZ9gA6",
        "outputId": "f5195ebd-f679-481c-f1f1-b650ea7812ae"
      },
      "source": [
        "remove_accented_chars(t3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Some Accented text'"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fvUZ5C2n9gA7"
      },
      "source": [
        "# Removing special characters, numbers & symbols"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gM6BwJs39gA8"
      },
      "source": [
        "import re\n",
        "\n",
        "def remove_special_characters(text, remove_digits=False):\n",
        "    pattern = r'[^a-zA-Z0-9\\s]' if not remove_digits else r'[^a-zA-Z\\s]' # retaining a-z, A-Z, 0-9 digits, and whitespace\n",
        "    text = re.sub(pattern, '', text)\n",
        "    return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "UELdM18Q9gA8",
        "outputId": "6df4b9b3-b495-4cf3-ad47-cabae6cc731d"
      },
      "source": [
        "t4 = \"Well this was fun! See you at 7:30, What do you think!!? #$@@9318@ 🙂🙂🙂\"\n",
        "t4"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Well this was fun! See you at 7:30, What do you think!!? #$@@9318@ 🙂🙂🙂'"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ND8E1Bg99gA9",
        "outputId": "fe7e37d8-fcbb-4ca3-cd7f-0effc0135b68"
      },
      "source": [
        "print(remove_special_characters(t4))\n",
        "print(remove_special_characters(t4, remove_digits=True))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Well this was fun See you at 730 What do you think 9318 \n",
            "Well this was fun See you at  What do you think  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r8NuFh7_9gA-"
      },
      "source": [
        "# Expansion & Contraction of statements"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "e3UU_QH69gA-",
        "outputId": "a12f758a-cbfe-417b-d80b-38d1c19ff500"
      },
      "source": [
        "t5 = \"Y'all can't expand contractions I'd think! You wouldn't be able to. How'd you do it?\"\n",
        "t5"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"Y'all can't expand contractions I'd think! You wouldn't be able to. How'd you do it?\""
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XYM5gDLk9gA_",
        "outputId": "fd87bbc9-c0b8-4fe0-97b2-a7040d64804d"
      },
      "source": [
        "import contractions\n",
        "\n",
        "list(contractions.contractions_dict.items())[:20]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(\"I'm\", 'I am'),\n",
              " (\"I'm'a\", 'I am about to'),\n",
              " (\"I'm'o\", 'I am going to'),\n",
              " (\"I've\", 'I have'),\n",
              " (\"I'll\", 'I will'),\n",
              " (\"I'll've\", 'I will have'),\n",
              " (\"I'd\", 'I would'),\n",
              " (\"I'd've\", 'I would have'),\n",
              " ('Whatcha', 'What are you'),\n",
              " (\"amn't\", 'am not'),\n",
              " (\"ain't\", 'are not'),\n",
              " (\"aren't\", 'are not'),\n",
              " (\"'cause\", 'because'),\n",
              " (\"can't\", 'cannot'),\n",
              " (\"can't've\", 'cannot have'),\n",
              " (\"could've\", 'could have'),\n",
              " (\"couldn't\", 'could not'),\n",
              " (\"couldn't've\", 'could not have'),\n",
              " (\"daren't\", 'dare not'),\n",
              " (\"daresn't\", 'dare not')]"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "CK0SpFQl9gBA",
        "outputId": "0700ec1a-4f49-4c45-c55f-fc02bcdda583"
      },
      "source": [
        "contractions.fix(t5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'you all cannot expand contractions I would think! You would not be able to. how did you do it?'"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eckWm63y_4ie"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HMAN6XoS_8tP"
      },
      "source": [
        "# Stemming"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gpqknZW3_7kt",
        "outputId": "485349cd-afab-4d8a-e8ca-cbe762604df4"
      },
      "source": [
        "# Using Porter_stemmer\n",
        "\n",
        "from nltk.stem import PorterStemmer\n",
        "ps = PorterStemmer()\n",
        "\n",
        "ps.stem('jumping'), ps.stem('drunked'), ps.stem('jumps'), ps.stem('grounded'), ps.stem('lying'), ps.stem('strange')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('jump', 'drunk', 'jump', 'ground', 'lie', 'strang')"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LYPmXnnRAN0X"
      },
      "source": [
        "### Available libraries: Porter_stemmer, Snowball_stemmer, Lancaster_stemmer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qRVlkOOqA71h"
      },
      "source": [
        "# Lemmatization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zMRqz8sCA-8O",
        "outputId": "3f63b9ef-dc2e-454d-d5b2-167889024de6"
      },
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "wnl = WordNetLemmatizer()\n",
        "help(wnl.lemmatize)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Help on method lemmatize in module nltk.stem.wordnet:\n",
            "\n",
            "lemmatize(word, pos='n') method of nltk.stem.wordnet.WordNetLemmatizer instance\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qH3P9IJSBIRO",
        "outputId": "61d2705e-aa64-4879-e5cd-92c8d6209d87"
      },
      "source": [
        "# Lemmatize NOUNS\n",
        "\n",
        "print(wnl.lemmatize('women','n'))\n",
        "print(wnl.lemmatize('houses','n'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "woman\n",
            "house\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z55a0jSVBVwP",
        "outputId": "78819f8d-d4bb-4409-a85f-4df2ccad9661"
      },
      "source": [
        "# Lemmatize VERBS\n",
        "\n",
        "print(wnl.lemmatize('playing','v'))\n",
        "print(wnl.lemmatize('taught','v'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "play\n",
            "teach\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zpLjMR2FBjOl",
        "outputId": "28a28c29-d445-4490-ffdb-ee5ac63fbdbd"
      },
      "source": [
        "# Lemmatize adjectives\n",
        "\n",
        "print(wnl.lemmatize('happiest','a'))\n",
        "print(wnl.lemmatize('brighter','a'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "happy\n",
            "bright\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kbr0Ng3NBwgO",
        "outputId": "a73d99cc-b1b9-4f19-8d7b-6e226279d3a6"
      },
      "source": [
        "# ineffective Lemmatization\n",
        "\n",
        "print(wnl.lemmatize('ate','n'))\n",
        "print(wnl.lemmatize('laziest','v'))\n",
        "print(wnl.lemmatize('fancier'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ate\n",
            "laziest\n",
            "fancier\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AAdfbNIYCCzT"
      },
      "source": [
        "# tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h5IfDb4pCGQI",
        "outputId": "2d040265-368a-44f1-f323-79abea8363f2"
      },
      "source": [
        "t6 = 'The brown foxes are quick and they are jumping over the sleeping lazy dogs!'\n",
        "tokens = nltk.word_tokenize(t6)\n",
        "print(tokens)\n",
        "print('\\n')\n",
        "lemmatized_text = ' '.join(wnl.lemmatize(token,'v') for token in tokens) # applying Lemmatization after tokenization\n",
        "print(lemmatized_text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['The', 'brown', 'foxes', 'are', 'quick', 'and', 'they', 'are', 'jumping', 'over', 'the', 'sleeping', 'lazy', 'dogs', '!']\n",
            "\n",
            "\n",
            "The brown fox be quick and they be jump over the sleep lazy dog !\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I7IggcAvCyZr"
      },
      "source": [
        "# POS( Part-of-Speech) tagging"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n7_g0FgKDSnD"
      },
      "source": [
        "#### Categorizing and then labelizing each word or symbol with a new LABEL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BAz-0m-gCf44",
        "outputId": "fb0ed659-374c-4bd0-be41-15d59303bf33"
      },
      "source": [
        "tagged_content = nltk.pos_tag(tokens)\n",
        "print(tagged_content)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('The', 'DT'), ('brown', 'JJ'), ('foxes', 'NNS'), ('are', 'VBP'), ('quick', 'JJ'), ('and', 'CC'), ('they', 'PRP'), ('are', 'VBP'), ('jumping', 'VBG'), ('over', 'IN'), ('the', 'DT'), ('sleeping', 'VBG'), ('lazy', 'JJ'), ('dogs', 'NNS'), ('!', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4-dSy7NPDcoF"
      },
      "source": [
        "### Tag conversion to WordNet Tags as Lemmatization works on this Dictionary nomenclature"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Fmfe2-6DDp_",
        "outputId": "5f116974-db37-4328-ded0-ace6831b7edb"
      },
      "source": [
        "from nltk.corpus import wordnet\n",
        "\n",
        "def pos_tag_wordnet(tagged_tokens):\n",
        "  tag_map = {'j':wordnet.ADJ, 'v': wordnet.VERB, 'n': wordnet.NOUN, 'r': wordnet.ADV} # denoting each label to corresponding wordnet deictionary POS\n",
        "  new_tagged_tokens = [(word, tag_map.get(tag[0].lower(), wordnet.NOUN)) for word, tag in tagged_tokens]\n",
        "  return new_tagged_tokens\n",
        "\n",
        "wordnet_tokens = pos_tag_wordnet(tagged_content)\n",
        "print(wordnet_tokens)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('The', 'n'), ('brown', 'a'), ('foxes', 'n'), ('are', 'v'), ('quick', 'a'), ('and', 'n'), ('they', 'n'), ('are', 'v'), ('jumping', 'v'), ('over', 'n'), ('the', 'n'), ('sleeping', 'v'), ('lazy', 'a'), ('dogs', 'n'), ('!', 'n')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m2IMXTk8Eujn"
      },
      "source": [
        "## Effective Lemmatization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "SUbbwbPIEyLD",
        "outputId": "167402a0-aee4-46c1-96f5-d3ab10e32c94"
      },
      "source": [
        "lemmatized_text = ' '.join(wnl.lemmatize(word,tag) for word, tag in wordnet_tokens)\n",
        "lemmatized_text"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The brown fox be quick and they be jump over the sleep lazy dog !'"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fgPEJxz6Fuxo"
      },
      "source": [
        "### Combining all the above steps into a single function\n",
        "\n",
        "\n",
        "1.   Function is wordnet_lemmatize_text\n",
        "2.   Input is variable_text which should be taken in a document( collection of words)\n",
        "3. Calling the above defined functions and utilizing them\n",
        "4. Returning lemmatized text as the output(i.e., as a STRING)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U0sFICyjGPDr"
      },
      "source": [
        "wnl = WordNetLemmatizer()\n",
        "\n",
        "def wordnet_lemmatize_text(text):\n",
        "  tagged_tokens = nltk.pos_tag(nltk.word_tokenize(text)) # tokenizing the sentences into words before [assing into the function\n",
        "  wordnet_tokens = pos_tag_wordnet(tagged_tokens)\n",
        "  lemmatized_text = ' '.join(wnl.lemmatize(word, tag) for word, tag in wordnet_tokens)\n",
        "  return lemmatized_text\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5iTsThZoHCSE"
      },
      "source": [
        "#### Testing on a sample case"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "e7iYxiEaHAPF",
        "outputId": "812eb123-91f5-4682-a849-e89e7ba1fccf"
      },
      "source": [
        "t6\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The brown foxes are quick and they are jumping over the sleeping lazy dogs!'"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "ePVDjDogHJmX",
        "outputId": "7caf4fae-5efb-46cc-c9fd-1bfd58994f4a"
      },
      "source": [
        "wordnet_lemmatize_text(t6)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The brown fox be quick and they be jump over the sleep lazy dog !'"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4bO8IHI-HVgs"
      },
      "source": [
        "## Lemmatization + SpacY"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LUxkTmGRHY1Z"
      },
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en', parse= False, tag=False, entity= False)\n",
        "\n",
        "def spacy_lemmatized_text(text):\n",
        "  text = nlp(text)\n",
        "  text = ' '.join([word.lemma_ if word.lemma_ != '-PRON' else word.text for word in text]) # operating on pronoun\n",
        "  return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "M8Z3cXutIVRU",
        "outputId": "492f9f8c-59e5-4bae-e094-335e006eea57"
      },
      "source": [
        "spacy_lemmatized_text(t6)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'the brown fox be quick and -PRON- be jump over the sleep lazy dog !'"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lhf2oa6eJBsw"
      },
      "source": [
        "## STOPWORD removal"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vvm0jtVsJAQ5"
      },
      "source": [
        "def remove_stopwords(text, is_lower_case= False, stopwords= None):\n",
        "  if not stopwords:\n",
        "    stopwords = nltk.corpus.stopwords.words('english') # importing ENGLISH stopwords list\n",
        "  tokens = nltk.word_tokenize(text)\n",
        "  tokens = [token.strip() for token in tokens]\n",
        "\n",
        "  if is_lower_case:\n",
        "    filtered_tokens = [token for token in tokens if token not in stopwords]\n",
        "  else:\n",
        "    filtered_tokens = [token for token in tokens if token.lower() not in stopwords]\n",
        "\n",
        "  filtered_text = ' '.join(filtered_tokens)\n",
        "  return filtered_text\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YOroWsTEKr0v",
        "outputId": "90ce2ec2-9a04-413c-8395-9208ba4e3615"
      },
      "source": [
        "# Let us see some of the stopwords available in the library\n",
        "stop_words = nltk.corpus.stopwords.words('english')  # predefined stopwords list, it is user-editable !!\n",
        "print(stop_words[:25])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "7OgrdBmpK23N",
        "outputId": "a795c49f-4bc4-457f-ec25-6a748ddb4e92"
      },
      "source": [
        "t6"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The brown foxes are quick and they are jumping over the sleeping lazy dogs!'"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "quWzkEmpK-89",
        "outputId": "aeed3d02-9d41-4e82-bc7b-d3aac2a076c2"
      },
      "source": [
        "remove_stopwords(t6, is_lower_case= False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'brown foxes quick jumping sleeping lazy dogs !'"
            ]
          },
          "metadata": {},
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8AQ_AYtBLa0Z"
      },
      "source": [
        "#### Removing word 'the' & add 'brown' to the predefined  STOPWORD list"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CzPKtaKOLYo4"
      },
      "source": [
        "stop_words.remove('the')\n",
        "stop_words.append('brown')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "PcBTdniJL2wm",
        "outputId": "65fe386d-38de-4d15-90fa-3546f5716aa5"
      },
      "source": [
        "remove_stopwords(t6, is_lower_case= False, stopwords= stop_words) # passing our-edited stopword list into the function"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The foxes quick jumping the sleeping lazy dogs !'"
            ]
          },
          "metadata": {},
          "execution_count": 93
        }
      ]
    }
  ]
}